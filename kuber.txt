# kuber_sum
----------
ГЛОССАРИЙ:
----------
Kubernetes — это платформа с открытым исходным кодом для управления контейнеризированными рабочими нагрузками (workload) и сервисами.
 Фактически Kubernetes является оркестратором рабочей нагрузки.

Namespace, неймспейс, пространство имен — это абстракция, которая позволяет разграничить объекты и рабочую нагрузку для разных команд или пользователей   за счет разных политик доступа к объектам Kubernetes и ограничения вычислительных ресурсов.

Node, нода — физическая или виртуальная машина, входящая в состав кластера Kubernetes.

Pod, под — это объект Kubernetes, который является описанием атомарной единицы рабочей нагрузки. Pod можно воспринимать,
 как описание запущенного инстанса сервиса или задачи.

Sidecar-контейнер в Kubernetes — это контейнер, который запускается рядом с основным контейнером внутри пода. Этот паттерн нужен для
 расширения и улучшения функциональности основного приложения без внесения в него изменений. 2
  Примеры использования sidecar-контейнеров:
   - сбор и передача логов из основного контейнера приложения; 4
   - сбор метрик из основного контейнера приложения и передача их в централизованную систему мониторинга; 4
   - синхронизация данных между основным контейнером приложения и внешним хранилищем или базами данных; 4
   - обнаружение сервисов и балансировка нагрузки; 4
   - обработка аутентификации с внешними системами. 4
 В большинстве случаев sidecar-контейнеры просты и легковесны и потребляют намного меньше ресурсов, чем основной контейнер.

Service, сервис — это объект Kubernetes, который описывает некоторый набор подов в качестве сетевого сервиса,
 а также способ доступа к этому сетевому сервису.

Deployment, деплоймент — это объект Kubernetes, который описывает в скольких экземплярах запущен сервис,
 а также стратегию обновления на новую версию.

Ingress, ингресс — это объект Kubernetes, в котором описываются правила маршрутизации клиентского трафика.

Service mesh в Kubernetes — это отдельный инфраструктурный уровень, который добавляется к приложениям для взаимодействия между
 сервисами или микросервисами. На этом уровне можно прозрачно добавить наблюдаемость, управление трафиком и безопасность, чтобы не включать их в код. 2
 Основные задачи service mesh:
  - гарантия доставки трафика до приложений (service mesh реализует retry, timeouts, circuit breaker и другие подобные механизмы);
  - увеличение безопасности при помощи шифрования трафика, верификации трафика на основе сертификатов, реализация дополнительных правил и разрешений на передачу трафика;
  - tracing запросов, составление схемы и визуализация трафика;
  - реализация переключения трафика при canary, a/b и blue/green стратегиях деплоя.
 Две самых популярных реализации service mesh — Istio и Consul.
 Service Mesh состоит из двух частей: control plane и data plane.
   - Data plane отвечает за исполнение сетевых политик, политик безопасности и сбор телеметрии. Чаще всего эта часть строится на базе сетевых прокси, которые запускаются рядом с приложением в формате sidecar-контейнера.
   - Control plane управляет data plane и отвечает за тиражирование политик, а также нередко включает в себя компоненты для сбора телеметрии. Control plane используется для управления маршрутизацией и балансировкой запросов, распространения ключей, секретов и в целом политик безопасности, например правил RBAC или ACL.
  Таким образом, контрольная панель превращает все data plane в распределённую систему. Она не касается никаких пакетов или запросов в системе.

Важно отметить, что это обобщённые определения, и на практике реализации Service Mesh могут отличаться.

API Server, апи сервер — это компонент управляющего слоя Kubernetes, который используется для управления кластером по API
 и взаимодействия внутренних компонентов.

Annotation, аннотации — это пары ключ-значение, в которых хранится значимая информация,
 которую используют сторонние инструменты или контроллеры для своей работы.

ControlPlane, управляющий слой, также в официальной документации может называться “панель управления” и “плоскость управления”
 — это набор управляющих компонент Kubernetes, которые отвечают за координацию и распределение рабочей нагрузки.
 Роль узла в кластере kubernetes - control-plane: Отвечает за управление кластером.

Kubelet — это агент Kubernetes, который запущен на всех нодах кластера, и осуществляет работу с контейнерным окружением:
 следит за живостью контейнеров, запускает новые контейнеры, ограничивает контейнеры по ресурсам и т.д.

Object, объект, ресурс – это хранящиеся внутри Kubernetes сущности. И Kubernetes их использует для представления состояния кластера.

Label, метка — это пары типа ключ-значение, которые связаны с конкретным объектом Kubernetes, и используются,
 чтобы указывать наборы объектов без необходимости фиксировать конкретные идентификаторы в спецификациях

Selector, селектор — это выражения, позволяющие выбрать объекты по меткам.

Controller, контроллер — это процесс, который пытается поддерживать в согласованном состоянии конфигурацию кластера и реальный мир.

Manifest, манифест — это файл с описанием объектов Kubernetes

ConfigMap, конфигмап — это объект Kubernetes, который хранит в себе конфигурацию

Secret, секрет — это объект, который предназначен для хранения чувствительной информации: например, логин пароль для подключения к базе данных.

Docker — (в частности, Docker Engine) — это программное обеспечение для виртуализации на уровне операционной системы,
 которая также известна как контейнеризация.

CRI, Container Runtime Interface, интерфейс среды выполнения контейнера — это API сред выполнения контейнера, которая интегрируется с kubelet на ноде.

CRI-O — легковесная среда выполнения контейнеров в Kubernetes.

----------------------------
ГЛАВНЫЕ ПРИНЦИПЫ KUBERNETES:
----------------------------
 - Неизменяемость: позволяет просто удалить неработающий узел (контейнер) - и он создастся заново;
   - Работает только в связке с какой-либо системой управления конфигурациями!
 - Декларативность: мы только указываем kubernetes какой должен быть результат - он сам занимается тем как его достич;
   - описываем в манифестах в yaml формате.
 - Самоисцеление: kubernetes сам отслеживает некорректно работающее приложение и перезапускает контейнер на котором оно работает.
 - Разделение: отдельно приложение, отдельно инфраструктура (ос, платформа);

--------------------------
ОСНВНЫЕ ЗАДАЧИ KUBERNETES:
--------------------------

- Запуск сервисов в контейнерном окружении и их распределение по серверам;
- Слежение за жизнеспособностью экземпляров сервисов;
- Конфигурирование сервисов;
- Управление средой выполнения сервисов и приложений;
- Осуществление различных стратегий обновления;
- Осуществление механизмов межсервисного взаимодействия;
- Осуществление механизмов балансировки и маршрутизации клиентского трафика;
- Управление хранилищами.

---------------
ОБЩЕЕ ОПИСАНИЕ:
---------------
Kubernetes разворачивается в кластере.

Кластер состоит из одной или нескольких физических, или виртуальных машин - ноды (или иногда встречается термин узлы).

В кластере есть 2 типа (роли) нод, которые отличаются только типом запущенной нагрузки:
 - управляющие ноды:
   на них - запущены компоненты управляющего слоя Kubernetes,
    отвечающие за координацию и распределение рабочей нагрузки по рабочим нодам.;
 - рабочие ноды:
   это машины, которые принимают нагрузку (полезную )и на которых запускаются сервисы (бизнес-приложения).

В тестовых или небольших инсталляциях Kubernetes роли могут совмещаться в одной ноде.

УПРАВЛЯЮЩАЯ НОДА - MASTER:
 - представляет собой набор компонентов Kubernetes, которые позволяют управлять кластером,
   в том числе взаимодействовать с кластером человеку.
   Минимальный набор таких компонентов:
   - etcd:
     Отказоустойчивое, децентрализованное и консистентное хранилище конфигурации и состояния кластера.
     Может работать в кластерном режиме - как минимум 3 экземпляра.
     Можно разворачивать как внутри так и вне кластера Kubernetes.
     Любые изменения в конфигурации кластера проходят через API Server и сохраняются в etcd.
     И только API Server напрямую ходит в etcd;
   - api-server:
     Для управления кластером и взаимодействия внутренних компонент Kubernetes. Все взаимодействие с кластером и между его компонентами
      осуществляется через API Server.
     В нем есть логика проверки формата запросов, аутентификации, проверки прав и т.д.
     API Server валидирует запрос (от пользователя или компоненты) и (при необходимости) производит изменения в конфигурации кластера.
     Так же, выполняет рассылку изменений конфигураций и состояния кластера.
     Другие компоненты подписываются на события, слушают их и обрабатывают,
      либо с какой-то периодичностью перечитывают конфигурацию через API Server.;
   - kube-scheduler:
     Компонент управляющего слоя (ControlPlane).
     Используя механизм оповещения об изменениях API Server-а, kube-scheduler узнает,
      что нужно запустить экземпляр сервиса и принимает решение о том, на какой ноде он должен быть запущен.
      И через API server обновляет состояние кластера;
   - kube-controller-manager:
      Компонент управляющего слоя.
      Представляет из себя - бинарный файл в который упакованы все базовые контроллеры Kubernetes,
       Обеспечивающие базовые функции самоуправления кластера.
      !Контроллер (controller) - это процесс, который пытается поддерживать в согласованном состоянии конфигурацию кластера и реальный мир.
      При изменении конфигурации кластера (etcd) контроллер пытается эти изменения отразить в реальном мире,
       и наоборот - если меняется состояние контейнеров - пытается вернуть его к состоянию описанному в конфигурации.
      Архитектура Kubernetes позволяет самому реализовать контроллеры.
      Иногда такие пользовательские контроллеры еще называют операторами (operator).

   - kubelet:
     Агент - для запуска полезной нагрузки на нодах.
     Запущен на каждой ноде: и управляющей, и рабочей.
     Читает событие с помощью API Server-a, что экземпляр сервиса распределен kube-scheduler-ом на ноду,
      на которой он работает, и запускает экземпляр сервиса.
     Например, если kubelet увидит, что экземпляр сервиса перестал работать,
      он через API Server обновит его состояние, чтобы другие компоненты могли отреагировать,
      и попытается его перезапустить, чтобы вернуть к жизни.

 РАБОЧАЯ НОДА:
  Включает в себя:
  - Для изоляции сервисов друг от друга - сервисы запускаются в контейнерном окружении,
     которое поддерживает спецификацию Container Runtime Interface (docker, cri-o и т. д.).;
  - kubelet;
  - kubeproxy:
    Агент, который обеспечивает внутреннюю адресацию сервисов, и способы доступа к ним.

----------------------------------
API И ОБЪЕКТЫ КЛАСТЕРА KUBERNETES:
----------------------------------
API Server - обеспечивает прикладное взаимодействие с кластером Kubernetes.
Консольная утилита Kubectl - работает через API Server.

Для передачи команд от администратора (оператора) API Server-у используется декларативный подход (описание конечного состояния).
 API Server-у передается часть конфигурации, которая описывает требуемое состояние.
 Далее контроллеры узнают об изменении этой части конфигурации и соответствующим образом реагируют.

Всю конфигурацию, а также описание состояние кластера,
 в Kubernetes для удобства и простоты разбили на набор описаний в виде объектов разных типов.
Объекты (objects)  – это хранящиеся внутри Kubernetes сущности. И Kubernetes их использует для представления состояния кластера.

Каждый объект хранит желаемое состояние или еще по-другому это называется спецификация,
 а также текущее актуальное состояние той части кластера, к которой они относятся.

Объекты могут описывать:
 - какие сервисы должны быть запущены и их актуальный статус;
 - какие вычислительные ресурсы (CPU и память) доступны сервисам;
 - политики поведения, такие как политика рестартов, обновлений и т.д.

Любая рабочая нагрузка: экземпляр сервиса, приложение или задача, имеет соответствующий ей объект Kubernetes,
 и этот объект находится в каком-то namespace-е (привязан к конкретному namespace).

Пространство имен разграничивает наборы объектов друга от друга и помогает различным проектам, командам,
 пользователям использовать один кластер Kubernetes.
 Разграничение происходит за счет разных пространств для имен объектов.
 Посредством namespace можно ограничивать использование вычислительных ресурсов (через квоты: максимальное значение памяти и CPU,
  которое может использовать суммарно вся рабочая нагрузка, относящаяся к namespace-у) и доступ к определенным namespace.
 Имена объектов Kubernetes должны быть уникальными среди объектов соответствующего типа не в рамках всего кластера Kubernetes,
  а только в рамках namespace-а, к которому они относятся.
 Это помогает командам, работающим в одном кластере не задумываться и не переживать насчет пересечения имен.
 Пространство имен — это абстракция - сервисы в одном namespace могут крутиться на разных узлах, и наоборот
  - на одном узле могут крутиться сервисы из разных namespace.
 Т.е. пространства имен в общем случае не соотносятся с инфраструктурным слоем кластера.
 Чаще всего в практике используют одну из следующих схем:
  - одно пространство имен на одну команду;
  - одно пространство имен на среду: prod, stage, qa;
  - одно пространство имен на одно приложение.

 Пространства имён не могут быть вложенными, а каждый ресурс Kubernetes может находиться только в одном пространстве имён.
 При удалении пространства имен происходит удаление всех объектов, которые находятся в нем.

Управление кластером и нагрузкой происходит через создание, изменение и удаление объектов.

Для этого в API Server-е используется REST подход
 - это архитектурный подход, служащий для организации взаимодействия приложений (или компонентов одного приложения) через сеть.

 То есть, каждому объекту соответствует ресурс в API и любые действия с объектами транслируются в действия с REST-ресурсами.
  И для создания ресурса нужно сделать POST запрос на URL коллекции,
   для получения ресурса GET на URL конкретного ресурса,
   для изменения — использовать метод PATCH,
   а удаления — метод DELETE. Поэтому иногда объекты называют еще ресурсами Kubernetes.

Изменения этих объектов — создание, удаление, обновление слушают различные контроллеры и производят реальные действия
 — запускают и тушат сервисы, перенастраивают сетевые политики, обновляют конфигурации балансеров и т. д.

Для встроенных объектов обычно есть соответствующий встроенный контроллер.

Когда создаются объекты (ресурсы), то просто добавляется "кусок конфигурации".
 Если контроллеров, реагирующих на изменения объектов нет, то такие объекты просто так и останутся данными,
  которые хранятся в хранилище, и не будут   иметь никакого эффекта в реальном мире.

Помимо встроенных объектов или ресурсов, Kubernetes позволяет добавлять пользовательские типы объектов (ресурсов).
 А также позволяет разрабатывать кастомные контроллеры сторонним разработчикам на изменения этих ресурсов.
 Это делает Kubernetes фреймворком, который легко можно расширять.

Описания объектов обычно хранят в yaml-файлах - манифестах Kubernetes.
 Как правило созданием манифестов занимается разработчик.

Каждый объект имеет ряд стандартных атрибутов:
 - apiVersion – версия API, которая вместе с типом объекта однозначно определяет конкретную схему объекта.
   Версия API нужна для развития схемы типа объекта.
   Это позволяет в одной версии Kubernetes-а поддерживать несколько разных форматов для одного типа объекта,
   совмещая устаревший формат и новый. Таким образом не ломается обратная совместимость старых манифестов и добавляется поддержка новых;
 - kind – тип объекта;
 - metadata – метаданные;
 - name - некоторый идентификатор объекта, по которому Kubernetes понимает, какой объект меняется.
   Имя должно быть уникальным среди объектов этого типа в пределах одного namespace!;
 - labels - метки и селекторы:
   Метки – это значения типа ключ-значение, которые связаны с конкретным ресурсом (объектом) Kubernetes.
    Метки определяют атрибуты объектов, которые имеют смысл для человека.
    Они могут быть прикреплены к объекту в момент создания и описываются в атрибуте .metadata.labels.
    Либо изменены в реальном времени - удалены, добавлены и т.д.
    Рекомендуется использовать логические (или семантические) ключи в метках, например,
     {app: myapp, tier: frontend, phase: test, deployment: v3}.
   Селекторы – это выражения, позволяющие выбрать объекты по меткам:
    Есть два типа селекторов:
     - основанные на совпадении (или не совпадении) меток (equality-based), описываются в разделе matchLabels;
     - основанные на наборах (set-based) фильтруют ключи в соответствии с набором значений.
       Поддерживаются три вида операторов: in, notin и exists. Описываются в matchExpressions.
       Пример:
--->
matchLabels:
  app: myapp
mathExpressions:
  {key: env, operator: In, values: [qa, dev]}
  {key: version, operator: In, values: [2]
<---

 - annotations - аннотации;
   Аннотации - как и метки, являются коллекциями с наборами пар ключ-значение.
   Они не используются для идентификации и выбора объектов!
   В отличие от меток, там хранится значимая информация, которую используют инструменты, библиотеки
    или контроллеры для своей работы.
   Аннотации располагаются в атрибутах metadata.annotations.
--->
metadata:
  name: myapp-service
  annotations:
    prometheus.io/scrape: 'true'
    prometheus.io/path: '/actuator/prometheus'
    prometheus.io/port: 8080
<---
 - spec – спецификация желаемого состояния объекта.  Для каждого типа объекта она своя;
 - status - текущие состояние объекта. Обычно не используется в yaml манифестах и используется внутри Kubernetes контроллера.

----------------------------
ОСНОВНЫЕ ОБЪЕКТЫ KUBERNETES:
----------------------------

POD:
---
Pod – это объект Kubernetes, который является описанием атомарной единицы рабочей нагрузки.
 Pod, как объект Kubernetes, является отражением реальных процессов, запущенных в кластере.
 При создании объекта типа Pod, Kubernetes запускает один или несколько контейнеров на одной из рабочих нод.
 При удалении - завершает работу этого контейнера, или контейнеров.

Контейнеры, описанные в поде, Kubernetes запускает, удаляет и обновляет, как единое целое.
В одном поде обычно запускают один основной контейнер, в котором находится бизнес-логика,
 а остальные контейнеры предоставляют, связанные функции, такие как мониторинг, логирование, конфигурация.

Как и у любого объекта Kubernetes, у него есть ряд общих для всех полей, это apiVersion, kind, metadata, spec.
 В spec находится спецификация пода.
 Pod - это набор контейнеров, поэтому в поле containers указывается массив с описанием контейнеров.

Как только создается новый объект типа Pod,
 информация об этом событии через API Server передается всем заинтересованным в этом компонентам,
 в том числе встроенным контроллерам.
Прежде всего информацию получает компонент kube-scheduler, и получив спецификацию пода он принимает решение о том,
 на какой ноде этот под будет реально запущен.
Событие, что конкретный под был распределен на ноду, получает компонент kubelet и уже дальше самостоятельно,
 обращаясь к API конкретной системы контейнеризации, запускает контейнеры.
При создании поду назначается уникальный IP адрес. Обратиться по этому IP адресу можно из любого пода,
 запущенного на любой из нод, а также с любой из нод кластера.

Фазы пода:
 - PENDING - как только под был созданб, и ни один контейнер не запущен;
 - RUNNING - как только хотя бы один контейнер запустился;
 - Succeeded - когда все контейнеры запустятся - успешно;
 - Failed - если хотя бы один из контейнеров запустился не успешно;
 - UNKNOWN - когда kubelet перестал сообщать статус пода в API Server;
 - Terminating - первая фаза при удалении пода.

Для корректного завершения подов kubelet старается реализовать механизм постепенной деградации (graceful degradation).
 Различные компоненты и контроллеры получают событие о завершении пода и производят соответствующие действия:
  убирают под из балансировки трафика и т.д.
 Также событие получает kubelet и начинает завершение пода на ноде.
  kubelet сначала посылает сигнал SIGTERM всем контейнерам пода и ждет некоторое время (т.н. graceful period),
   по умолчанию это 30 секунд.
  Если в течение этого времени, контейнер не завершился, то kubelet шлет SIGKILL и удаляет под с помощью API server-а.
  Время ожидания можно настроить, но это время не гарантировано Kubernetes-ом, и иногда возможны ситуации,
   когда контейнер убивается за меньшее время.
  Поэтому крайне желательно на уровне приложения предусматривать такие ситуации и уметь их корректно обрабатывать.

Kubelet следит за состоянием контейнеров, относящихся к поду, и в случае проблем может предпринимать действия для их исправления.
Kubelet может периодически производить следующие проверки контейнеров, связанных с подом:
 - livenessProbe:
   проверка на то, что контейнер жив и приложение работает.
   Если проверка не прошла, то kubelet убивает контейнер и в зависимости от настроек, может перезапустить его.
 - readinessProbe - проверка на способность контейнера отвечать на запросы.
   Если проверка не прошла, то тогда под убирается из балансировки, и запросы к нему не идут.
 - startupProbe - проверка на то, запустился контейнер или нет.
   Если проверка не прошла, то тогда kubelet убивает контейнер и, в зависимости от настроек, может перезапустить контейнер.
 Настройки и параметры проверок описываются в атрибутах livenessProbe, readinessProbe и startupProbe спецификации контейнера.
 У kubelet-а есть 3 основных способа проверки:
  - ExecAction - выполнение команды внутри контейнера. Если команда завершилась с кодом 0, то проверка считается успешной
  - HTTPGetAction - выполнение http запроса по определенному порту и урлу.
    Если http статус код ответа больше или равен 200 и меньше 400, то тогда проверка считается успешно пройденной
  - TCPSocketAction - выполнение tcp запроса по определенному порту. Если порт открыт, то проверка считается успешно пройденной.

По умолчанию контейнеры могут использовать все доступные на ноде вычислительные ресурсы: CPU и оперативную память.
 С помощью квот на ресурсы администраторы могут установить ограничение для конкретных контейнеров внутри пода.
 Чаще всего ставятся ограничения на CPU и оперативную память.
 Есть два типа ограничений:
  - запросы (request) - запрашиваемые ресурсы.
    Это те ограничения по ресурсам, которые использует kube-scheduler для того, чтобы принять решение о том,
     на какую ноду можно распределить под.
    Также kubelet на ноде резервирует системных ресурсов для этого контейнера не меньше, чем указано в запросах.

  - лимиты (limit) - максимально доступные ресурсы.
    Это те ресурсы, за которые не дает выйти kubelet и контейнерное окружение.
    Например, если контейнер попробует использовать больше памяти, чем у него указано в лимитах,
     ядро его может убить с ошибкой "Out Of Memory".

  Ограничения указываются в спецификации контейнера в опциональном атрибуте resources.
  Ресурсы описываются в следующих единицах:
   - CPU описывается в долях процессорного времени, выделенного для контейнера.
     1 CPU- это эквивалентно 1 vCPU/Core для облачных провайдеров
      и 1 виртуальное ядро (гипертред) для железных серверов на архитектуре Intel.
     Допускается использование дробных значений. Например, можно указать 0.1 или 100m.
     Суффикс m означает милликор - 0.001 CPU. Т.е. записи 0.1 и 100m эквивалентны. Меньше 1m указать нельзя.

   - Память по умолчанию описывается в байтах, но также можно использовать суффиксы K, М, G для указания кило, мега и гигабайт.
     Или в степенях двойки соответственно - Ki, Mi, Gi.

 Пример yaml файла пода:
--->
apiVersion: v1
kind: Pod
metadata:
  name: hello-demo
spec:
  containers:
  - name: hello-demo
    image: schetinnikov/hello-app:v1
    ports:
      - containerPort: 8000
<---

DEPLOYMENT:
-----------
Deployment – это объект Kubernetes, который описывает в скольких экземплярах запущен сервис,
 а также стратегию обновления на новую версию.

Контроллер, который реализует поведение этого объекта, встроен в Kubernetes, и работает в рамках компонента kube-controller-manager.
 Встроенный контроллер деплоймента решает следующие задачи:
  Следит, чтобы количество доступных подов было равно требуемому (значение параметра replicas).
  Обеспечивает процесс обновления версии приложения.

Как и у любого объекта Kubernetes, у него есть ряд общих для всех полей, это apiVersion, kind, metadata, spec.
 В spec находится спецификация деплоймента:
  - replicas — желаемое количество экземпляров сервиса
  - selector — селектор для подов, которые будут находиться под управлением деплоймента
  - strategy — стратегия обновления и ее настройки
  - template — это шаблон пода, по которому будут создаваться поды

При создании объекта типа Deployment, встроенный контроллер поднимает нужное количество подов, используя шаблон из спецификации.
 Если мы изменим спецификацию объекта Deployment, и поменяем параметр replicas, например, с 2 до 3,
  то тогда деплоймент обнаруживает, что желаемое количество реплик отличается от реального,
  и добавит еще один под по шаблону. Похожий процесс происходит и при даунскейле, т.е. если уменьшить количество реплик.
 Если одна из реплик перестает работать - Deployment - запустит новую.

При любом изменении в шаблоне, встроенный контроллер начинает процесс обновления деплоймента.
 Для этого все поды со старым шаблоном удаляются, а с новым шаблоном — создаются.
 Встроенный контроллер поддерживает 2 стратегии обновления:
  - Recreate - пересоздание.
    Сначала все старые поды удаляются. Как только поды были удалены, создаются новые в нужном количестве.
    Такая стратегия приводит к простою приложения, поэтому используется редко.
  - RollingUpdate - постепенное обновление.
    Удаляются старые поды и создаются новые постепенно, так, чтобы в любой момент времени несколько под были доступны.
    У стратегии RollingUpdate есть две настройки, которые позволяют регулировать,
     сколько подов минимально должно быть доступно,
     и на сколько мы можем превысить количество подов во время обновления:
      - spec.strategy.rollingUpdate.maxUnavailable - это опциональное поле в описании настроек стратегии RollingUpdate.
        Оно описывает максимальное количество подов,
         которое может быть недоступно во время процесса обновления в виде процента от общего количества,
         либо в абсолютном выражении.
        Например, если значение этого параметра выставлено в 30%, то это означает,
         что контроллер сразу же может потушить 30% старых под, не дожидаясь, пока поднимутся новые,
         и, как только новые поды буду подниматься и готовы принимать трафик,
         контроллер и дальше может старые тушить, оставляя общее количество доступных под не меньше 70%.
      - spec.strategy.rollingUpdate.maxSurge - это опциональное поле в описании настроек стратегии RollingUpdate.
        Оно описывает, на какое количество под контроллер может превысить желаемое значение реплик
         (экземпляров) во время обновления.
        Оно может выражаться как в процентах от общего количества, так и в абсолютном выражении.
        Например, если значение этого параметра выставлено в 30%, то это означает,
         что контроллер может, не дожидаясь момента, как будут удалены старые поды,
         создать новых под столько, чтобы общее количество старых и новых не превышало 130% от желаемого числа под.

  Пример yaml Deployment:
--->
api Version: apps/v1
kind: Deployment
metadata:
  name: hello-deployment
spec:
  replicas: 2
  selector:
    matchLabels:
      app: hello-demo
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: hello-demo
    spec:
      containers:
        - name: hello-demo
          image: 'schetinnikov/hello-app:v2'
          ports:
            - container Port: 8000
<---

SERVICE:
--------
Service – это объект Kubernetes, который описывает некоторый набор подов в качестве сетевого сервиса,
 а также способ доступа к этому сетевому сервису.

В самом простом случае создается некоторый "виртуальный" IP, при обращении на который трафик балансируется по соответствующим подам.
 Реализуется это с помощью с встроенного контроллера в Kubernetes и компонента kube-proxy,
  который производит изменения в правилах маршрутизации трафика на нодах кластера.

Как и у любого объекта Kubernetes, у него есть ряд общих для всех полей, это apiVersion, kind, metadata, spec.
 В spec находится спецификация сервиса:
  - selector — определяет, какой набор под попадет под управление сервиса
  - ports.port — порт сервиса, запросы на который будут проксироваться на ports.targetPort
  - ports.targetPort — порт пода, на котором отвечает приложение
  - type — типа сервиса. Определяет каким образом можно получить доступ к сервису:
    - ClusterIP — по внутреннему IP в кластере
    - NodePort — по порту на ноде кластера
    - LoadBalancer — по какому-то внешнему, по отношению к кластеру, IP.

Встроенный в Kubernetes контроллер слушает события о создании, удалении и обновлении объектов типа Service.
 Как только создается новый объект Service, контроллер находит свободный "виртуальный" IP адрес из подсети и назначает его Service-у.
  Также события обновления, создания и удаления объектов типа Service слушает kube-proxy на каждой ноде.
  Как только объекту Service назначен внутренний IP адрес (он также называется ClusterIP),
   kube-proxy прописывает правила в сетевой файервол (iptables),
   которые все обращения на CluserIP сервиса маршрутизируют на конкретные IP адреса подов.
  Поды для балансировки выбираются с помощью селектора, указанного в спецификации.
  В реализации по умолчанию балансировка равномерная, т.е. все поды будут получать примерно одинаковое количество запросов.
  Поскольку kube-proxy запущен на всех нодах, то изменения в правилах буду применены на всех нодах кластерах.

При создании объекта типа Service никаких новых процессов или балансировщика на рабочих нодах не запускается.
 Лишь меняются правила маршрутизации трафика на ноде.
 В случае, если добавляется еще один под, подходящий под селектор, то kube proxy обновляет правила.

Пример yaml Service:
--->
apiVersion: v1
kind: Service
metadata:
  name: hello-service
spec:
  selector:
    app: hello-demo
  ports:
    - port: 9000
      target Port: 8000
  type: ClusterIP
<---

Т.к. внутренний ClusterIP обычно назначается случайным образом системой, а не устанавливается заранее,
 то необходим механизм, подсказывающий виртуальный IP сервиса.
 В Kubernetes есть два механизма обнаружения сервисов.
 Первый основан на переменных окружения и мало кем используется,
  а вот второй работает с помощью системы доменных имен (DNS) и является де-факто стандартом.
 Большинство дистрибутивов Kubernetes предоставляют реализацию DNS сервиса,
  как правило, на основе coredns, либо его можно поставить самостоятельно.
  В этом случае при создании сущности Service создается запись в DNS
   и обеспечивается механизм разрешения доменных имен из подов внутри кластера Kubernetes.
  Из подов, которые находятся в том же самом пространстве имен, к сервису можно обратиться просто по его имени - {service-name}.
   Доменное имя {service-name} внутри пода будет резолвится в IP сервиса.
   Если под находится в другом namespace, то тогда можно обратиться по доменному имени {service-name}.{namespace}.
   Также можно использовать полноценное доменное имя типа - {service-name}.{namespace}.{cluster-name}

Тип ClusterIP дает возможность обращаться к набору подов, как сетевому сервису, только внутри кластера Kubernetes.
 В случае, если мы хотим получить доступ к сервису извне кластера, мы можем использовать другие типы сервисов.

NodePort — самый простой способ направить внешний трафик в сервис.
 Предоставляет не только внутренний IP внутри кластера, но и закрепляет за конкретным сервисом определенный порт на всех нодах.
 Обычно выбирается свободный порт из диапазона 30000-32000,
  но диапазон может отличаться для разных реализаций сетевого плагина в Kubernetes.
  Можно самому указать порт, но рекомендуют дать Kubernetes выбрать самому.
 Для того чтобы получить доступ извне должен существовать некоторый внешний,
  по отношению к кластеру Kubernetes, балансировщик, который балансирует трафик по рабочим нодам,
  и в правилах маршрутизации используются порты на нодах.
 Не рекомендуется использоватьв критичных продакшн кластерах среднего и большого размера!!!

LoadBalancer:
 Помимо порта на нодах и внутреннего IP, также назначается внешний IP адрес, по которому будет доступен сервис.
 Для работы этого типа сервисов требуется наличие специального, обычно облачного, контроллера,
  который реализует логику выбора и назначения IP.
 Если мы создадим сервис с типом LoadBalancer, в инсталляции Kubernetes без соответствующего контроллера,
  то внешний ip у сервиса не будет назначен.

Помимо LoadBalancer-а и NodePort-a, для того, чтобы внешний клиентский трафик попадал в кластер,
 можно использовать сущности Ingress и GatewayAPI.

INGRESS:
--------
Ingress — это объект Kubernetes, в котором описываются правила маршрутизации клиентского трафика,
 например запросов веб приложения в браузере.

Ingress относится к тому типу объектов, для которых нет встроенных контроллеров.
 Более того, в рамках кластера Kubernetes может жить несколько ингресс контроллеров.

Ингресс контроллер всегда связан с каким-то реальным балансировщиком,
 который принимает клиентский трафик.
 Это может быть nginx, haproxy, envoy или какой-то api gateway, типа ambassador-а.
 И задача контроллера проста: он читает изменения объектов типа Ingress,
  и применяет эти правила к конкретной конфигурации балансера.
 Т.е. чаще всего работа заключается в том, чтобы изменить конфиги балансировщика и отправить сигнал, чтобы он их перечитал.

Ингрессы не умеют работать с протоколами отличными от HTTP или HTTPS (например, WebSocket).
 Для работы с этими протоколами предполагается использование сервисов типа LoadBalancer и NodePort.

Как и любой объект он содержит атрибуты apiVersion, kind, metadata, spec.
 В метаданных есть аннотации. Объекты типа Ingress могут обрабатываться несколькими ингресс-контроллерами.
  В аннотации может быть указан тип балансира с которым взаимодействует ingress, например nginx.
 В спецификации описаны правила маршрутизации - rules.

На текущий момент Ingress является слабо расширяемой сущностью,
 и от нее планируется уйти в сторону сущностей GatewayAPI, которые находятся в активной разработке.

Пример файла ingress.yaml:
--->
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: hello-ingress
  annotations:
    kubernetes.io/ingress.class: "nginx"
spec:
  rules:
  - http:
      paths:
        - path: /
          pathType: Prefix
          backend:
            service:
              name: hello-service
              port:
                number: 9000
 <---

CONFIGMAP, SECRET:
------------------
Для того чтобы передать в приложение конфигурацию, можно использовать переменные окружения.
 В спецификации контейнера это можно сделать через атрибут env.
env - массив пар типа ключ-значение, где ключом является имя переменной окружения, а значением - ее значение.

Для того чтобы отделить конфигурацию приложения от среды запуска приложения, в Kubernetes есть два специальных ресурса:
 - ConfigMap - для хранения конфигураций;
 - Secret - для хранения потенциально чувствительной информации (например, логин пароль для подключения к базе данных).

Secret отличается от ConfigMap только тем, что данные в нем хранятся по умолчанию в закодированном (base64),
 но не зашифрованном виде.
Т.е. если кто-то получит доступ к Secret-у, он его сможет легко раскодировать.
Как правило, разработчики и инженеры сами не создают секреты в продакшн средах,
 только используют секреты, которые для них подготовили администраторы.
Доступ к секретам также ограничивается на основе ролевой модели в Kubernetes.
Таким образом, разработчики и инженеры могут использовать секреты в манифестах, но не знают, какие данные там хранятся.
А кодирование в base64 нужно, т.к. иногда чувствительные данные представляют собой бинарные данные, которые в yaml хранить сложно.

Все секреты и конфигмапы привязываются к конкретному пространству имен.

Использовать созданные ConfigMap и Secret возможно следующим образом:
 - передавать значения из ConfigMap/Secret в виде переменных окружения;
 - передавать значения из Configmap/Secret в виде примонтированной директории внутрь контейнера.

Configmap и Secret выглядят следующим образом. Атрибут data содержит данные в формате ключ-значение.
Примеры:

--->
apiVersion: v1
kind: ConfigMap
metadata:
  name: hello-config
data:
  GREETING: Privet
<---

--->
apiVersion: v1
kind: Secret
metadata:
  name: hello-secret
data:
  DATABASE_URI: cG9zdGdyZXNxbCtwc3ljb3BnMjovL215dXNlcjpwYXNzd2RAcG9zdGdyZXMubXlhcHAuc3ZjLmNsdXN0ZXIubG9jYWw6NTQzMi9teWFwcA==
<---
-------------------
УСТАНОВКА MINIKUBE:
-------------------
Можно не устанавливать локально, а зайти в labs.play-with-k8s.com

1. Устанавливаем docker.

2. Устанавливаем консольную утилиту kubectl.
https://kubernetes.io/docs/tasks/tools/#kubectl

3. Устанавливаем minikube.
https://minikube.sigs.k8s.io/docs/start/

; Запуск minikube:
minikube start

----------------------
ИСПОЛЬЗОВАНИЕ KUBECTL:
----------------------

; Справка kubectl:
kubectl <command> --help

; Посмотреть доступные в kubernetes ресурсы:
kubectl api-resources

; Получение описания объекта определенного типа
kubectl explain [pod|replicaset|deployment|...]

; Получить информацию о запущенных узлах на кластере:
kubectl get node

; Получить информацию о ноде в кластере:
kubectl describe node

; Получить информацию о статусе управляющих компонент:
kubectl get componentstatus

; Создать namespace:
kubectl create namespace <name_ns>

; Изменить текущее пространство имен (namespace), чтобы каждый раз не передавать в параметрах -n:
kubectl config set-context --current --namespace=<name_ns>

; Создать под (из файла pod.yml):
kubectl create -f pod.yml -n <name_ns>

; Создать под или обновить (из файла pod.yml):
kubectl apply -f pod.yml -n <name_ns>

; Посмотреть поды:
kubectl get pod -n <name_ns>
    -A        # покажет во всех namespace

; Посмотреть yaml файл пода:
kubectl get pod -o yaml

; Посмотреть файл пода в формате json:
kubectl get pod -o json

; Посмотреть расширенную информацию о поде:
kubectl describe pod <pod_name> -n <name_ns>

; Посмотреть расширенную информацию о поде в табличном формате:
kubectl get pod -o wide

; Вывод подов по имени приложения:
kubectl get pod -l app=<app_name>

; Также можно получить несколько ресурсов за раз, перечислив их через запятую, например:
kubectl get no,ns

; Посмотреть логи пода:
kubectl logs <pod_name> -n <name_ns>

; Выполнение команды внутри пода:
kubectl exec -it <pod_name> <command>

; Включить форвардинг порта пода:
kubectl port-forward pod/<pod_name> -- <host_port>:<pod_port>

; Удаление объекта:
kubectl delete [pod|replicaset|deployment|...] <object_name>
 --wait=false        # исключает ожидание корректного завершения пода;

; Удаление всех объектов:
kubectl delete [pod|replicaset|deployment|...] --all

; Удаление всех объектов (не включая ингрессы и конфигмапы):
kubectl delete all --all

; Удаление объектов объявленных в файле:
kubectl delete -f file.yaml

; Создание деплоймента из cli:
kubectl run –image <image_name>:<tag_name> [command]

; Создание/изменение деплоймента из конфигурации:
kubectl apply -f deployment.yaml

; Получить информацию о деплойменте:
kubectl get deploy <deploy_name>

; Масштабировать деплоймент на определенное количество реплик:
kubectl scale deploy/<deploy_name> --replicas=2

; Изменение объекта на лету:
kubectl edit [pod|replicaset|deployment|...] <object_name>

; Обновление имэджа:
kubectl set image [deployment|...] container=<image>:<tag>

; Просмотр файла в контейнере
kubectl exec <container_name> cat /etc/config.conf

; Проксирование API Server на локальный порт 8080:
kubectl proxy --port=8080

; Получить конфигурацию и состояние кластера, относящуюся к нашей ноде:
curl -s 127.0.0.1:8080/api/v1/nodes/$HOSTNAME/ | jq

; С помощью формата вывода jsonpath в kubectl можно доставать любую информацию о поде.
  Это крайне полезно и удобно для работы в скриптах.
  Например, можем вывести только IP пода hello-demo такой командой:
  kubectl get -o jsonpath='{.status.podIP}' pod <pod_name>
  ; сохраним IP пода в переменную POD_ID:
  POD_IP=$(kubectl get -o jsonpath='{.status.podIP}' pod hello-demo)
  ; по этому IP мы можем обратиться к поду:
  curl http://$POD_IP:8000/
